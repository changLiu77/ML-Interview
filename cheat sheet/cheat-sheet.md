

## Subpage

| URL | description | Title |
| --- | --- | --- |
| domain.md | Related knowledge for specific job positions  | Specific domain knowledge for position |
| recSys.md | Simplified for Recommandation sys | Deep Learning Rec Sys |
| infrustructure.md | techniques about large model infratructure: distributed, comparession .etc | LM Infrastructure |
| transformer_lm.md | Detailed description of transformer + techniques for language model | Transformer + Language Model |
| basic.md | Basic knowledge and conceptions about ml and dl | Basic ML + DL |



# Index


[Basic ML + DL](basic.md) 

## **ML**


## **DL**


[Transformer + Language Model](transformer_lm.md) 

## Transformer


- Basic Structure
- Different transformers structures
## Large Model Tricks 


- MoE (Mixture of Expert)
- Fine-tuning 
    - Peft
    - LoRA
    - Adaptor
    - Instruct tuning
    - Prompt tuning

- RAG 
[LM Infrastructure](infrustructure.md) 

[Deep Learning Rec Sys](recSys.md) 

[Specific domain knowledge for position](domain.md) 

## Multi-Modality Model 


### **CLiP **


![Image](./media/image_1b505834-30de-8037-bee9-cfc97356b605.png)


- Align text with image 
    - The similar text-image pair will be close with each other 
    - Mismatch pair will be away from each other 

- Train 
    - self-supervise: only need right text-image pair 
    - Train image and text encoder â‡’ them will be reflected to same vector space 
    - Trained by contrastive Loss: similiarity between each pair  

- Inference
    - Give text / image â‡’ embedding â‡’ retrieve the topk similar from given images / texts description 
        - Can used for **search engine **


### **Blip**


- VLM 
- Train 
    - Trained elements
        - An image encoder 
        - An image-grounded encoder for text â‡’ better on select / retrieve text for image 
        - An image-grounded decoder for text â‡’ better on caption image 

    - Contrstive Loss (image - text) (ITC)
        - similar to clip 

    - Match loss / classification loss ( image -text match)(ITM)
        - **classify whether it is the right text **
```math
L_{ITM} = -YlogP - (1 - Y)log(1-P)
```


        - Train a **filter for text**
        - Image embedding in **cross attention **

    - language loss 
        - the same as GPT, the probability of next word 
```math
L = -\sum{log(P(\omega_t|\omega_{i<t}))}
```


        - Train a **text generator **

    - Two stage training
        - coarse train: on a low-q dataset: self-supervised â‡’ still use all losses
        - One specific task / dataset: VQA, caption etc. 


![Image](./media/image_1b505834-30de-80da-a6c9-ca068131cc56.png)


![Image](./media/image_1b505834-30de-8055-aaa6-f07000bf39a5.png)


### **Blip2**


- Like Blip1 , but 
    - Align with LLM 
        - Add a fully connected layer to align
        - Loss : vision-language

    - Compress two encoders â‡’ use Q-Former instead 
        - One **visual representation learner: **
            - Original: Image-text + Image-grounded -text 
            - Only learn **a small subset of embeddings**

        - One text-decoder: similar to original decoder 


![Image](./media/image17_1b505834-30de-8047-9ebf-e1ccf8b25d2c.png)


![Image](./media/image13_1b505834-30de-8075-8efe-f7d5d813f403.png)


- Train: two stage
    - First stage: coarse: only Q-Former like Blip1 training 
    - Second stage: on specific dataset, both Q-Former and fc layer

### Instruct-BLiP


![Image](./media/image_1c405834-30de-80c4-aae2-ccc789ae8dce.png)


- Other part is the same with **Blip2 **
- Only add another **intruction tuning **stage: replace the original input text with **instruction style input**
### **LLaVA**


- A multi-modal **reasoning-enabled model**
- Has training parts
    - Clip encoder
    - A projection layer to align text with image
    - LLaMA: used for text generate

- Training: pretrain + **instruction-based training **
    - pretrain: only the **projection layer** is trained â‡’ align the image with text 
        - Large dataset

    - instruction-based
        - The **LLM + projection is trained**
        - dataset: generated by gpt: input instruct â†’ related text â†’ align image with **generated text (a triplet)**
        - Use language model loss: the probability of pridicting the next token
![Image](./media/image_1b505834-30de-80b6-a6a9-f55f88babd26.png)




- LLaVA 1.5, LLaVA1.6: 
    - better clip encoder: use ViT(better version)
    - More various dataset

## Diffusion 


- Diffusion vs gan
    - More stable and can learn more general data distribution 
    - More expansive

### Basic


**DDPM**

- Forward: Add noises step by step to the input image
- Backward: pridict noise **reversely by sampling**. It is a **stocastic estimation**
    - Depend on previous **sampling**** xt **
    - Gaussian noise sampling

- What is noise scheduler:  noise variance ð›½ð‘¡ over ð‘‡ steps
    - In inference, will add a  **random noise at output**
        - Each time outputs / generated figures are different 
        - Make the process stocastic â‡’ more accurate learn 


- score function: learned the **distribution of data**. It is learned to match the distribution of data 
![Image](./media/1742512715608_d_1bc05834-30de-806f-a055-f2c7cfad4b98.png)


- DDIM:
    - difference: forward is the same. Backward is **computed instead of sampling and estmated**
        - Only add noise at **certain steps **
        - Not adding any **random variables **during both training and inference so results are always the same 


- How to speed up
    - DDIM
    - Latent Diffusion: compress to a **lower dimension space**
    - Distillation: train a small set
    - More efficient noise scheduler like **cosine noise**

- What is conditioned diffusion, difference between classifer-free classifer-based
    - Add more information
    - classifer-based: **pretrained network â‡’ **gradient for diffusion 
    - classifer-free: **interpolote the result **w/t info

- SDXL vs flux 
    - flux: transformer encoder-based â‡’ More Global 
    - SDXL: partial transformer based â‡’ **attention based**

### ControlNet


- A hard modulate over output: Like add a **modulator in each output feature map**
- Add layers of `1x1 conv` + `copy trained nueral layers` to learn hard mask 
- Trained parameters
    - zero-init conv: init as 0 to  **avoid adding noise at start **
    - copied layer: all layers in **encoder **
        - The encoder block will use **corresponding zeor init conv and added to correspoinding decoder **


- Advantage
    - Precise 
    - Support multiple conditioning  

- Disadvantage 
    - Expensive: make inference slower even after trained 
    - Memory intensive: one - one â‡’ have to store motiply resutls

![Image](./media/2bff698f99ef8960264e52492de83a6_1bc05834-30de-8098-b737-d10dad016824.jpg)


![Image](./media/image_1bc05834-30de-8073-a7e7-f5afa25754d3.png)


- IP Adaptor: add image embedding into  **another cross attention module **
    - Will change the feature map output of generation â‡’ a **soft modulate of generation **
    - The generation will have the similar subject / style as input 
