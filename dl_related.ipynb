{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Manipulaation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-k and indices from batch of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input: `batch_size, num_elements`\n",
    "* output: largest k along the last dimension + indices \n",
    "* Use for **retrieve mission**\n",
    "\n",
    "Follow up \n",
    "\n",
    "* any dimension \n",
    "* the smallest k  \n",
    "* do not use topk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: `topk` \n",
    "* largest_k_value, indices = torch.topk(tensor, k, dim, largest=True)\n",
    "\n",
    "Function: `sort`\n",
    "* sorted_values, sorted_index = torch.sort(tensor, dim, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(tensor, k, largest=True):\n",
    "    k_value, k_index = torch.topk(tensor, k, largest=largest, dim=-1)\n",
    "    return k_value, k_index \n",
    "\n",
    "def top_k_slice(tensor, k, largest=True):\n",
    "    sorted_tensor, sorted_index = torch.sort(tensor, dim=-1, descending=largest)\n",
    "    return sorted_tensor[..., :k], sorted_index[..., :k]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all negetive in a tensor with mean of positive \n",
    "\n",
    "* input: `batch_size, num_elements`\n",
    "* output: `batch_size, num_elements` \n",
    "* Use for **handling special values / denoising data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: `where` \n",
    "* tensor = torch.where(condition, value-if-true, value-if-false)\n",
    "* An **element wise** replacement of value with provided value \n",
    "* Use to replace the value based on **condition**\n",
    "  * Replace invalid value \n",
    "  * Select values \n",
    "\n",
    "Function `masked_fill` \n",
    "\n",
    "* tensor = tensor.masked_fill(mask, val)\n",
    "* Do a **element wise** replacement where the mask **is True**\n",
    "\n",
    "Tensor: mask \n",
    "\n",
    "* Element wise comparison \n",
    "* To generate a mask of the same size: tensor > val \n",
    "* A boolean mask => Can used for other computation \n",
    "  * extract all `mask==1` element: `tensor[mask]`\n",
    "    * The output is of `1D` All dimension lost !! \n",
    "  * Applying the mask `tensor * mask` => the dimension is not changed \n",
    "\n",
    "Function computation, like `sum` \n",
    "\n",
    "* tensor = tensor.sum(dim, keepdim)\n",
    "  * Boolean value can also be applied\n",
    "* Do the computation along the given dim\n",
    "* if `keepdim` is True `B, L, E` => `B,L,1` else `B,L` \n",
    "  * The computed dim is simply one umber \n",
    "\n",
    "Function: clamp\n",
    "\n",
    "* tensor = tensor.clamp(min=val1, max=val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_neg_with_mean(tensor):\n",
    "    # get negetive mask\n",
    "    negetive_mask = tensor < 0 \n",
    "    # only where > 0 will used for compute averate \n",
    "    sum_positive_tensor = tensor.masked_fill(negetive_mask, 0).sum(dim=-1, keepdim=True)\n",
    "    avg_positive_num = (~negetive_mask).sum(dim=-1, keepdim=True).clamp(min=1)\n",
    "    replace_tensor = sum_positive_tensor / avg_positive_num \n",
    "    replaced_tensor = torch.where(negetive, replace_tensor, tensor)\n",
    "    return replaced_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange, Reshape and Flatten a tensor \n",
    "\n",
    "* Input: a tensor `Batch, C, H, W` \n",
    "* Output: a tensor that is reshaped / rearranged / flatten\n",
    "* Use for: many computation needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between `permute`, `view`, `transpose`\n",
    "\n",
    "* permute: \n",
    "  * **reordering of any dimensions**\n",
    "  * `tensor = tensor.permute(0,4,3,2,1)` => B,L,C,H,W => B,W,H,C,L \n",
    "  * Do not copy data, doesn't need the data to be contigeous \n",
    "* transpose:\n",
    "  * swap **only two dimensions**\n",
    "  * `tensor = tensor.transpose(-2, -1)` => B,L,H,W => B,L,W,H\n",
    "* view(The only shared memory operation):\n",
    "  * Change the **shape** without changing order in memory => Memory mush be **contigeous** + new shape must match the input shape \n",
    "  * `tensor = tensor.view(L1, L2, L3)`\n",
    "  * Faster than permute\n",
    "  * Directly change the ***memory***\n",
    "  * The created tensor **share same memory** with original => change one will influence another \n",
    "* reshape\n",
    "  * Change the **shape** of tensor \n",
    "  * `tensor = tensor.reshape(l1, l2...)`\n",
    "* flatten \n",
    "  * convert the multiple dim vector from start dim=> **1d** tensor \n",
    "  * `tensor = tensor.flatten(start_dim)`\n",
    "    * if `start_dim` is not defined: B,H,W => B * H * W \n",
    "    * if `start_dim=1`, B,H,W => B, H * W\n",
    "* squeeze\n",
    "  * **remove all dim == 1**, not only a specific dim \n",
    "  * `tensor = tensor.squeeze()` \n",
    "* unsqueeze \n",
    "  * add a **new dimension** at new given dim / for **broadcasting**\n",
    "  * `tensor = tensor.unsqueeze(0)` H,W => 1,H,W "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a batch of grayscale images stored in a 4D tensor of shape `(batch_size, 1, height, width)`.\n",
    "Each image has `one channel (1)`, but we want to process them as flattened 2D matrices (removing the channel dimension).\n",
    "Your task is to **reshape** the images into a batch of 2D tensors of shape (batch_size, height, width) and then:\n",
    "- Flatten each image into a vector of shape (batch_size, height Ã— width)\n",
    "- Convert back into the original shape using a reshaping function.\n",
    "- Return both the flattened images and the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_process_img(tensor):\n",
    "    squeeze_tensor = tensor.squeeze() # B, H, W\n",
    "    B, H, W = squeeze_tensor.shape()\n",
    "    flatten_tensor = tensor.view(B, H * W).clone()\n",
    "    # or \n",
    "    flatten_tensor = tensor.flatten(start_dim=1)\n",
    "    reconstructed_tensor = flatten_tensor.view(B, 1, H, W)\n",
    "    return flatten_tensor, reconstructed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode \n",
    "\n",
    "* input: `B` (if a one-dimension result) / `B * H * W` a image \n",
    "* output: `B * class` / `B * class * H * W` \n",
    "* used for: cross entropy, used as `y^`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: `scatter_` \n",
    "* fill the vector if the value with the indices\n",
    "* tensor = tensor.scatter_(dim, reference, 1)\n",
    "  * if the input is [1,2], the vector to be filled is [[0,0,0], [0,0,0]], result is [[0,1,0], [0,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_one_dim_encode(tensor, num_class):\n",
    "    # input: B \n",
    "    # output: B, N\n",
    "    B = len(tensor)\n",
    "    expended_tensor = torch.zeros((B, num_class))\n",
    "    expended_tensor.scatter_(1, tensor.unsqueeze(1), 1)\n",
    "    return expended_tensor\n",
    "def one_hot_multi_cls_encode(tensor, num_class):\n",
    "    # input: B,n (n is the class label for each b)\n",
    "    # output: B, N\n",
    "    B, _ = tensor.shape()\n",
    "    one_hot = torch.zeros((B, num_class))\n",
    "    one_hot.scatter_(1, tensor, 1)\n",
    "    return one_hot \n",
    "def one_hot_image_encode(tensor, num_class):\n",
    "    # input: B, H, W \n",
    "    # output: B, N, H, W \n",
    "    B, H, W = tensor.shape()\n",
    "    one_hot = torch.zeros((B, num_class, H, W))\n",
    "    one_hot.scatter_(1, tensor.unsqueeze(1), 1)\n",
    "def one_hot_multi_channel_encode(tensor, num_class):\n",
    "    # input: B, C, H, W \n",
    "    # output: B, C*N, H, W \n",
    "    B, C, H, W = tensor.shape()\n",
    "    one_hot = torch.zeros((B, C, num_class, H, W))\n",
    "    one_hot.scatter_(2, tensor.unsqueeze(2), 1)\n",
    "    one_hot = one_hot.view((B, num_class*C, H, W))\n",
    "    return one_hot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract values from tensor based on index mapping\n",
    "\n",
    "* input: a tensor + index mapping\n",
    "* output: result from tensor and reconstruct following index \n",
    "* use for: nlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `gather`\n",
    "* Extract value in certain dim with given index \n",
    "* `new_tensor = torch.gather(tensor, dim, index)`\n",
    "\n",
    "Function `chunk` \n",
    "* chunk the tensor into several pieces along dimension \n",
    "* `new_tensor = torch.chunk(tensor, chunk_num, dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_value(tensor, dim, index):\n",
    "    return torch.gather(tensor, dim, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch matrix / matrix multiplication\n",
    "\n",
    "* `torch.triu(input, diagonal)`\n",
    "  * diagonal = 0: the upper triangle \n",
    "  * diagonal = 1: the upper triangle without diagonal\n",
    "  * diagonal = -1: the upper triangle add one disgonal below\n",
    "* `torch.diag(input, diagonal)`\n",
    "  * diagonal definition is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement a weighted sum of a tensor along an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(tensor, weight, dim):\n",
    "    # if weight applied along dim1\n",
    "    weighted_tensor = tensor * weight.unsqueeze(1)\n",
    "    return weighted_tensor.sum(dim=dim, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Load and Pre-process \n",
    "\n",
    "* Initialize: a self-defined dataset supports\n",
    "  * load a batch of image: need to load the correspoinding figures when called \n",
    "  * on-the-fly image processing  \n",
    "  * A dataloader for **pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset`\n",
    "* An API wraps all operations when a batch of data is loaded \n",
    "* Need to initialize\n",
    "  * How to load the figure \n",
    "  * After load the figure, the needed value for training and their formats\n",
    "    * figure\n",
    "    * label \n",
    "    * others if needed\n",
    "  * pre-process, like change them to tensor + augmentation \n",
    "    * `tranforms`: pre-process defined officially `torchvision.transforms` \n",
    "      * `ToTensor`: must be used to transform to tensor\n",
    "      * `Resize`, `RandomRotation`, `RandomHorizontalFlip` `ColorJitter` like this olny applies to `PIL`\n",
    "        * `Resize`: Resize((shape))\n",
    "        * `RandomRotation`: RandomRotation(degrees)\n",
    "        * `RandomHorizontalClip`: RandomHorizontalClip(p)\n",
    "        * `RandomCrop`: RandomCrop(size)\n",
    "      * `Normalize`: only apply to `tensor` \n",
    "  * when loading\n",
    "    * `__get_item__(self, idx)`: load each item, will aggeregate to a batch later => must be able to **concatenate**\n",
    "      * Can use `PIL` to load image\n",
    "      * Can convert to RGB\n",
    "    * `__len__(self)`: how many figures\n",
    "`DataLoader`\n",
    "* Used to load **batch of data** from dataset, only need to pay attention to \n",
    "  * How to connect with the dataset\n",
    "  * How to load: `for step, batch in enumerate(dataloader)`, batch is the loaded item\n",
    "  * Number of batches: len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "# * import place is different, in utils.data\n",
    "import torchvision.transforms and transforms \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from Image import PIL\n",
    "class NewDataset(Dataset):\n",
    "    def __init__(self, img_path, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform \n",
    "    def __len__(self):\n",
    "        return len(self.img_path) \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_path, idx)\n",
    "        img = PIL.open(img_path).convert('RGB')\n",
    "        # * apply transform\n",
    "        if self.transform is not None:\n",
    "            img_processed = self.transform(img)\n",
    "        return img_processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(img_path, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.RandomHorizontalClip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(contrast=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    ])\n",
    "    dataset = NewDataset(img_path, transform=transform)\n",
    "    \n",
    "    \n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML and DL Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression \n",
    "\n",
    "Show the flow of train a model\n",
    "\n",
    "* define the model, optimizer \n",
    "* Manually\n",
    "  * Compute loss => `loss.backward()` => in `no_grad`, update parameters, `p.grad.zero_()`\n",
    "* by torch\n",
    "  * compute loss => clear computed grad `zero_grad()` => `loss.backward` => `optimizer.step()`\n",
    "\n",
    "#### Linear Regression \n",
    "\n",
    "* Input: input $x$, label $y$ parameters (with / without optimizer)\n",
    "* Loss: MSE Loss \n",
    "* Output: progression of training: updated weight \n",
    "  * weight update \n",
    "\n",
    "#### Logistic Regression \n",
    "* Loss: BCE Loss / cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(x, y_true, lr, epochs):\n",
    "    # STEP1: initialize W and b \n",
    "    W = torch.randn(x.shape[1:], requires_grad=True)\n",
    "    b = torch.randn(y.shape[1:], requires_grad=True)\n",
    "    \n",
    "    # STEP2: train \n",
    "    for epoch in epochs:\n",
    "        # STEP3: compute y \n",
    "        y = X @ W + b \n",
    "        \n",
    "        # STEP4: compute loss \n",
    "        MSE_loss = ((y - y_true) ** 2).mean()\n",
    "        \n",
    "        # * if sigmoid regression \n",
    "        y = torch.sigmoid(y)\n",
    "        BCE_loss = - (y_true * log(y) + (1-y_true) * log(1 - y)).mean()\n",
    "        # STEP5: compute gradients \n",
    "        MSE_loss.backward()\n",
    "        \n",
    "        # STEP6: manually compute upgrade\n",
    "        with torch.no_grad():\n",
    "            W = W - lr * W.grad \n",
    "            b = b - lr * b.grad \n",
    "            # STEP7: zero out the gradient\n",
    "            W.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, class_num):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, class_num)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_linear_torch(X, Y, lr, epochs):\n",
    "    B, D = X.shape \n",
    "    B, N = Y.shape \n",
    "    linearLayer = LinearRegressionModel(D, N)\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = nn.BCELoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(linearLayer.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y = linearLayer(x)\n",
    "\n",
    "        loss = criterion(y, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.forward()\n",
    "    return linearLayer\n",
    "\n",
    "    # ! inference\n",
    "    # ! binary class \n",
    "    y_pred = x @ W + b \n",
    "    logits_pred = x @ W + b \n",
    "    \n",
    "    y_pred = torch.sigmoid(logits_pred) > 0.5 \n",
    "    \n",
    "    # ! multi class \n",
    "    y_pred = torch.argmax(y_pred, dim = 1)\n",
    "    y_pred = torch.argmax(torch.softmax(logits_pred, dim = 1), dim = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Block\n",
    "\n",
    "* input: X, Y true, num_classes, lr, epochs\n",
    "* output: trained model \n",
    "* Model\n",
    "  * multi layer: different linear layer \n",
    "  * Activation\n",
    "  * Other connection layer: dropout, Norm ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, num_class):\n",
    "        # * if not defined layer by layer\n",
    "        prev_dim = input_dim \n",
    "        self.layer = []\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layer.append(nn.Linear(prev_dim, hidden_size)),\n",
    "            self.layer.append(nn.BatchNorm(hidden_size)),\n",
    "            self.layer.append(nn.ReLU()),\n",
    "            self.layer.append(nn.Dropout(0.1))\n",
    "            prev_dim = hidden_size\n",
    "        self.layer.append(nn.Linear(prev_dim, num_class))\n",
    "        self.model = nn.Sequential(*self.layer)\n",
    "        \n",
    "        # or \n",
    "        self.layer = nn.ModuleList()\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layer.append(nn.Sequential([\n",
    "                nn.Linear(prev_dim, hidden_size),\n",
    "                \n",
    "            ]))\n",
    "    def forword(x):\n",
    "        return self.model(x)\n",
    "    \n",
    "def train_mlp(self, X, y_pred, lr, epochs):\n",
    "    B, D = X.shape()\n",
    "    B, N = y_pred.shape()\n",
    "    \n",
    "    mlp = MLPBlock(D, [16, 128, 32], N)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        y = mlp(X)\n",
    "        loss = criterion(y, y_pred)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return mlp \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP without autograd(backward)\n",
    "\n",
    "* Use chain rule to compute the weight manually \n",
    "* After define the loss, the grad of each parameter\n",
    "* compute new W based on **gradient descending updates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_single_layer(X, y_pred, epochs, lr):\n",
    "    B, D = X.shape()\n",
    "    B, N = y_pred.shape()\n",
    "    \n",
    "    W = torch.randn((D, N))\n",
    "    b = torch.randn((1, N))\n",
    "    for epoch in range(epochs):\n",
    "        # B, N \n",
    "        y = X @ W + b \n",
    "        y_act = F.ReLU(y)\n",
    "        \n",
    "        # can remove \n",
    "        loss = ((y_act - y_pred) ** 2).mean()\n",
    "        # B, N \n",
    "        dL_dy = 2 * (y_act - y_pred) / B \n",
    "        dya_dy = (y > 0)\n",
    "        # B, D \n",
    "        dy_dw = X \n",
    "        # \n",
    "        dW = dy_dw.T @ (dL_dy * dya_dy).float() \n",
    "        db = (dL_dy * dya_dy).float().sum(dim=0, keepdim=True)\n",
    "        \n",
    "        W = W - lr * dW \n",
    "        b = b - lr * db \n",
    "    return W, b \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add self-defined (e.g:ReLU) in MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GeLU\n",
    "\n",
    "* Gaussian enhanced ReLU: solved the abrupt change of Relu from 0 -> infinite\n",
    "* Always used in **transformer**\n",
    "* Dis: computational expensive\n",
    "\n",
    "* $ GeLU = x cdf(x) = 0.5(1 + tanh(\\sqrt{\\frac{2}{\\pi}}) \\times (x + 0.044715 x^3))$\n",
    "\n",
    "#### SiLU \n",
    "\n",
    "* smoother activate\n",
    "* smoother gradient flow \n",
    "* Used in **normalization layer** and **transformer / diffusions**\n",
    "* $ SiLU = x \\times \\sigma(x) = x \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "    def forward(x):\n",
    "        return torch.where(x > 0, x, 0)\n",
    "\n",
    "class LeakyReLU(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "        self.alpha = alpha \n",
    "    def forward(x):\n",
    "        return torch.where(x > 0, x, x * self.alpha)\n",
    "\n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeLU, self).__init__()\n",
    "    def cdf(self, x):\n",
    "        return 0.5 * (1 + torch.tanh(torch.sqrt(2 / torch.pi) * (x + 0.044715 * x ^^ 3)))\n",
    "    def forward(x):\n",
    "        return x * self.cdf(x)\n",
    "\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "    def forward(x):\n",
    "        return x * F.sigmoid(x)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, act_fn):\n",
    "        self.layer = nn.Sequential([\n",
    "            nn.Conv2d(input_c, output_c),\n",
    "            act_fn(),\n",
    "            nn.BatchNorm2d(output_c),\n",
    "            nn.Dropout(0.5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (manually)\n",
    "\n",
    "All Normalization follows \n",
    "\n",
    "* $ scale \\frac{x - \\mu}{\\sigma + \\epsilon} + shift $\n",
    "\n",
    "#### Batch Norm\n",
    "\n",
    "* `scale` and `shift` is learable per batch \n",
    "* compute ber mini-batch \n",
    "* Stablize training \n",
    "\n",
    "#### Layer Norm\n",
    "\n",
    "* `scale` and `shift` is learable per layer \n",
    "* compute per layer \n",
    "* Always used in NLP \n",
    "\n",
    "#### Adaptive Layer Norm\n",
    "\n",
    "* `scale` and `shift` are computed by control signal \n",
    "* How it is controled is determined by the specific AdaLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_embeddings, epsilon=1e-5, momentum=0.9):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_embeddings))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_embeddings))\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_embeddings))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_embeddings))\n",
    "    def forward(self, x):\n",
    "        # along the batch \n",
    "        if self.training:\n",
    "            mean = x.mean(dim=(0,2,3), keepdim=True)\n",
    "            var = x.var(dim=(0,2,3), unbiased=False)\n",
    "            self.running_mean = self.running_mean * self.momentum + mean * (1 - self.momentum)\n",
    "            self.running_var = self.running_var * self.momentum + var * (1 - self.momentum)\n",
    "            return self.scale * (x - mean) / torch.sqrt(var + self.epsilon) + self.shift\n",
    "        else:\n",
    "            return self.scale * (x - self.running_mean) / torch.sqrt(self.running_var + self.epsilon) + self.shift\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_embeddings, epsilon): \n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_embeddings))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_embeddings))\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.epsilon) + self.shift\n",
    "    \n",
    "class AdaLN(nn.Module):\n",
    "    def __init__(self, , num_embeddings, output_dim=None, time_emb_num=None, epsilon=1e-5):\n",
    "        # if input is not embedding\n",
    "        if time_emb_num is not None:\n",
    "            self.emb = nn.Embedding(time_emb_num)\n",
    "        # * output space\n",
    "        output_dim = output_dim if output_dim is not None else num_embeddings * 2\n",
    "        # * Used for reflect input embeddings to another space\n",
    "        self.linear = nn.Linear(num_embeddings, output_dim)\n",
    "        # * Used for activation\n",
    "        self.act = nn.SiLU()\n",
    "        self.norm = nn.LayerNorm(output_dim // 2, epsilon)\n",
    "    def forward(self, x, emb, temb=None):\n",
    "        if self.emb is not None:\n",
    "            temb = self.emb(emb)\n",
    "        # * reflect to other space\n",
    "        temb = self.linear(self.act(temb))\n",
    "        # * slice the results to two vectors\n",
    "        scale, shift = temb.chunk(2, dim = 0)\n",
    "        norm = self.norm(x)\n",
    "        return (1 + scale) * norm + shift \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ac, Recall, Prec, Confusion Matrix \n",
    "\n",
    "* Input: true label (B, N), pridicted label(B, N)\n",
    "* Output: matrix (B * N * N), each ele is the number of pair (True, Pridict)\n",
    "* Use for classification problem: TP, FP, FN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(num_classes, pridicted, true):\n",
    "    N = len(pridicted)\n",
    "    confuse_mat = torch.zeors((num_classes, num_classes), dtype=torch.int64)\n",
    "    \n",
    "    # row is true, col is pridict \n",
    "    # unique (true, pridicted) pair\n",
    "    # (1,0,2) , (1,1,2) => (4, 1, 8)\n",
    "    index = num_classes * true + pridicted\n",
    "    # count from (0,0) to (N-1, N-1) => how many pairs\n",
    "    cf_mat = torch.bincount(index, minlength=num_classes ** 2)\n",
    "    confuse_mat = cf_mat.view(num_classes, num_classes)\n",
    "    \n",
    "    return confuse_mat\n",
    "\n",
    "TP = cf_mat.diag()\n",
    "FN = cf_mat.sum(dim=1) - TP\n",
    "FP = cf_mat.sum(dim=0) - TP\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Loss\n",
    "\n",
    "#### MSE\n",
    "\n",
    "* minimun square loss \n",
    "* $ \\sum (Y - Y_t)^2 $\n",
    "\n",
    "#### MAE \n",
    "\n",
    "* minimum absloute loss \n",
    "* $ \\sum |(Y - Y_t)| $\n",
    "\n",
    "#### BCE \n",
    "\n",
    "* Binary cross entropy \n",
    "* $ - y_t * log(y) - (1 - y_t) * log(1 - y) $\n",
    "\n",
    "#### Multi-class: cross entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "* Input: True Probability(B, N), Pridicted label (B)\n",
    "* Output: loss \n",
    "* Loss = $\\sum(y_{true} * log(y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pridicted, true):\n",
    "    # softmax of pridicted (log)\n",
    "    logits = F.softmax(pridicted, dim=-1)\n",
    "    log_logits = torch.log(logits)\n",
    "    \n",
    "    one_hot = torch.zeros_like(pridicted)\n",
    "    one_hot = torch.scatter_(1, true.unsqueeze(1), 1)\n",
    "    \n",
    "    return -sum(one_hot * log_logits, dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss\n",
    "\n",
    "* Use for clip or multi-modality model\n",
    "* Always used for aligning different modality \n",
    "  * Pairs distance \n",
    "* Step\n",
    "  * Normalize along batch (l2) `F.normalize(tensor, p=2, dim=-1)\n",
    "  * similarity: $cosine sim = \\frac{E_i E_t}{temperature}$ => Between each pair is achieved by `matmul`\n",
    "  * Get the gt label pair\n",
    "  * Compute **cross entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_emb, text_emb, temperature=0.7):\n",
    "    # Normalize \n",
    "    # image B, D text: B, D \n",
    "    image_emb_normalized = F.normalize(image_emb, p=2, dim=-1)\n",
    "    text_emb_normalized = F.normalize(text_emb, p=2, dim=-1)\n",
    "    \n",
    "    logits = torch.matmul(image_emb_normalized, text_emb_normalized.T) / temperature \n",
    "    \n",
    "    gt = torch.arange(image_emb.shape[0])\n",
    "    \n",
    "    img_txt = F.cross_entropy(logits, gt)\n",
    "    text_img = F.cross_entropy(logits.T, gt)\n",
    "    \n",
    "    return img_txt, text_img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU and NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    box format: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # Compute intersection\n",
    "    inter_width = max(0, x2 - x1)\n",
    "    inter_height = max(0, y2 - y1)\n",
    "    intersection = inter_width * inter_height\n",
    "\n",
    "    # Compute union\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    if union == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "\n",
    "    return intersection / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, scores, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    boxes: list of [x1, y1, x2, y2]\n",
    "    scores: list of confidence scores\n",
    "    iou_threshold: IoU threshold for suppression\n",
    "    \"\"\"\n",
    "    # Sort boxes by descending score\n",
    "    indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    keep = []\n",
    "\n",
    "    while indices:\n",
    "        current = indices.pop(0)\n",
    "        keep.append(current)\n",
    "\n",
    "        new_indices = []\n",
    "        for i in indices:\n",
    "            iou = compute_iou(boxes[current], boxes[i])\n",
    "            if iou < iou_threshold:\n",
    "                new_indices.append(i)  # Keep boxes with low IoU\n",
    "\n",
    "        indices = new_indices\n",
    "\n",
    "    return keep  # indices of boxes to keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Operation \n",
    "\n",
    "#### Manually Operation \n",
    "\n",
    "#### Use unfold to perform \n",
    "\n",
    "* Use to reshape the input tensor to **slices**, which is the same as the **kernel size**\n",
    "* It is more **GPU friendly**: after unfold **the input**, when it multiples with **kernel**, direct **multiplication** is enough, no need to move + weighted sum\n",
    "* `tensor = F.unfold(tensor, kernel_size=()), stride, padding` (other conv parameters)\n",
    "  * `N, C, H, W` * `KH, KW` => `N, C * KH * KW, out_H * out_W` \n",
    "  * `out_H` = output size = (2 * padding + H - kH + 1) // stride\n",
    "\n",
    "#### Casual Conv \n",
    "\n",
    "Only the down triangle can be used. Other part can not be used => set the **filtered place as mask**\n",
    "\n",
    "#### Use Conv to implement pooling operation\n",
    "\n",
    "Average pooling is averaging in a square == **all 1 kernel** conv / kernel_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_conv(input_tensor, kernel, stride, padding):\n",
    "    B, C, H, W = input_tensor.shape()\n",
    "    C_out, _, kH, kW = kernel.shape()\n",
    "    \n",
    "    out_H = (H - kH + 1 + 2 * padding) // stride + 1\n",
    "    out_W = (W - kW + 1 + 2 * padding) // stride + 1\n",
    "\n",
    "    output_tensor = torch.zeros((B, C_out, out_H, out_W))\n",
    "    padded_input = F.pad(input_tensor, (padding, padding, padding, padding))\n",
    "    \n",
    "    for i in range(out_H):\n",
    "        for j in range(out_W):\n",
    "            # * B, C_in, kH, kW\n",
    "            window = padded_input[:, :, i * stride: i * stride + kH, j * stride: j * stride + kW] \n",
    "            # * B, 1, C_in, kH, kW\n",
    "            window_expand = window.unsqueeze(1)\n",
    "            res = window_expand * kernel.unsqueeze(0)\n",
    "            # * B, C_out, 1, 1\n",
    "            output_tensor[:,:,i,j] = (res).sum(dim=2, 3,4)\n",
    "    return output_tensor\n",
    "def unfold_conv(input_tensor, kernel, stride, padding):\n",
    "    B, C, H, W = input_tensor.shape()\n",
    "    out_channels, in_channels, kH, kW = kernel.shape()\n",
    "    \n",
    "    output_H = (H - kH + 1 + 2 * padding) // stride \n",
    "    output_W = (W - kW + 1 + 2 * padding) // stride \n",
    "    # * B, C * kH * kW, out_W * out_H \n",
    "    folded_input = F.unfold(input_tensor, kernel_size=(kH, kW), stride=stride, padding=padding)\n",
    "    # * out_channels, C * kW * kH\n",
    "    kernel_rearange = kernel.view(out_channels, -1)\n",
    "    output = folded_input * kernel_rearange\n",
    "    return output.view((N, out_channels, output_H, output_W))\n",
    "\n",
    "class Causal_3d_Conv(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, stride, padding, kernel):\n",
    "        super(Causal_3d_Conv, self).__init__()\n",
    "        self.conv_3d = nn.Conv3d(output_channel, input_channel, kernel, stride, padding)\n",
    "        \n",
    "    def forward(x):\n",
    "        mask = x.tril(torch.ones_like(x))\n",
    "        return mask * self.conv_3d(x)\n",
    "    \n",
    "class Conv_pooling(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv_pooling, self).__init__()\n",
    "        self.conv_kernel = torch.ones((kernel_size, kernel_size)) / (kernel_size ** 2)\n",
    "        self.stride = kernel_size \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape()\n",
    "        return F.conv2d(x, self.conv_kernel, stride=self.stride, groups=C)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer \n",
    "\n",
    "* conv \n",
    "* Activation\n",
    "* BatchNorm\n",
    "  * Stable training\n",
    "  * Tensor normalization \n",
    "* Dropout\n",
    "  * overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, out_channels, in_channels, kernel_size, padding, stride, pool_size):\n",
    "        self.layer = nn.Sequential([\n",
    "            nn.Conv2d(out_channels=out_channels, in_channels=in_channels, kernel_size=kernel_size, padding = padding, stride= stride),\n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size),\n",
    "            nn.Dropout(0.5)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE \n",
    "\n",
    "Encoder => Reparameterization => Decoder \n",
    "\n",
    "Reparameterization: needs to resample from current x distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channel=3, hidden_dim, latent_dim):\n",
    "        self.encoder = nn.Sequential([\n",
    "            nn.Conv2d(out_channels=hidden_dim, in_channels=input_channel, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels=hidden_dim * 2, in_channels=input_channel, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm(hidden_dim * 2), \n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential([\n",
    "            nn.ConvTranspose2d(output_channels=hidden_dim, in_channels=hidden_dim * 2, kernel_size = 4, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(output_channels=hidden, in_channels=input_channel, kernel_size = 4, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(input_channel),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "        \n",
    "        self.mu_linear = nn.Linear(hidden_dim * 2 * 8 * 8, latent_dim)\n",
    "        self.mu_logvar = nn.Linear(hidden_dim * 2 * 8 * 8, latent_dim)\n",
    "        self.decoder_fc = nn.Linear(latent_dim, hidden_dim * 2 * 8 * 8)\n",
    "    def reparameterize(self, x, mu_x, logvar_x):\n",
    "        gaussian_distribute_x = torch.randn_like(x)\n",
    "        \n",
    "        var = exp(logvar_x / 2)\n",
    "        \n",
    "        return (gaussian_distribute_x - mu_x) * var \n",
    "    def forward(self, x):\n",
    "        encoded_x = self.encoder(x)\n",
    "        # to latent space\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        mu_x = self.mu_linear(x)\n",
    "        logvar_x = self.mu_logvar(x)\n",
    "        repara_z = self.reparameterize(x, mu_x, logvar_x)\n",
    "        decode_x = self.decoder_fc(repara_z)\n",
    "        decode_x = x.view(x.shape[0], hidden_dum * 2, 8, 8)\n",
    "        return self.decoder(decode_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "See another file about transformer, Multi-head_Attention and sin-cos-position encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Modality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of LoRA Model and Application of Transformer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinearLayer(nn.Linear):\n",
    "    def __init__(self, input_dim, output_dim, rank, alpha):\n",
    "        super().init__()\n",
    "        self.r = rank \n",
    "        self.alpha = alpha\n",
    "        self.Lora_W_down = nn.Linear(input_dim, r)\n",
    "        self.Lora_W_up = nn.Linear(r, output_dim)\n",
    "        \n",
    "        nn.init.normal_(self.Lora_W_down.weight, std = 1/rank)\n",
    "        nn.init.normal_(self.Lora_W_up.weight, std = 1/rank)\n",
    "    def forward(self, x):\n",
    "        out = super().forward(x)\n",
    "        return self.alpha * self.Lora_W_up(self.Lora_W_down(x)) + out \n",
    "\n",
    "for names, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if \"attentions.key\" in names or ..:\n",
    "            lora_layer = LoRALinearLayer(module.in_features, module.out_features, rank, alpha)\n",
    "            setattr(name, module, lora_layer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The whole process of Diffusion model \n",
    "\n",
    "#### Each step \n",
    "\n",
    "(Used both in inference and training): The deducuction is in note\n",
    "\n",
    "* **Note**: All $\\alpha_t$ there is **cumulative product of noise scale of $1 - \\beta_t$**\n",
    "\n",
    "In both training and inference, one step will include remove noise to original figure \n",
    "\n",
    "* The output is the **pridicted noise**\n",
    "* Update rules $x_{t - 1} = x_{t} - \\alpha_t * model(x_t, t)$\n",
    "\n",
    "Training will also **add noise**, which depends on the **noise scheduler**\n",
    "\n",
    "#### The whole process\n",
    "\n",
    "* Iteratively denoise\n",
    "* Set default steps (until the input noise is removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward process \n",
    "\n",
    "* reversed backward: from `xt` to `x0` \n",
    "* Update rules $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\sqrt{1-\\alpha_t}\\epsilon_{\\theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Used funtions**\n",
    "\n",
    "* `cumprod`: cumulative product of given variables (include t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    def __init__(self, num_steps, beta_start, beta_end, sample_steps):\n",
    "        # * beta linear variance \n",
    "        self.num_steps = num_steps\n",
    "        self.sample_steps = sample_steps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps)\n",
    "        self.alphas = 1.0 - self.betas \n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        self.timestemps = self.get_timesteps()\n",
    "    def get_timesteps(self):\n",
    "        pass \n",
    "        # * depend on ddim / ddpm\n",
    "    def add_noise(self, x, noise, t):\n",
    "        alpha_t = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return x * alpha_t.sqrt() + noise * (1 - alpha_t).sqrt()\n",
    "    def remove_noise(self, x, pridicted_noise, t):\n",
    "        alpha_t = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        beta_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        \n",
    "        removed = (x - pridict_noise * (1 - alpha_t).sqrt() ) / alpha_t.sqrt()\n",
    "        \n",
    "        # * if DDPM: add random noise\n",
    "        z = torch.randn_like(x)\n",
    "        return removed + beta_t * z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(model, noise_scheduler, steps, img_size):\n",
    "    # STEP1: input noise \n",
    "    x = torch.randn((1, *img_size))\n",
    "    \n",
    "    # * Steps is a list containing the target denoised step\n",
    "    for t in range(steps):\n",
    "        pridict_noise = model(x, torch.tensor[t])\n",
    "        # * Add a \"minus noise\" to the figure\n",
    "        x = noise_scheduler.remove_noise(x, pridict_noise, t)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, noise_scheduler, steps, img, loss, optimizer):\n",
    "    for t in range(steps):\n",
    "        noise = torch.randn_like(img)\n",
    "        noised_img = noise_scheduler.add_noise(img, noise, t)\n",
    "        \n",
    "        pridicted_noise = model(noised_img)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss(noised_img - pridicted_noise, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a noise Scheduler in Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function of Noise Scheduler \n",
    "\n",
    "* Time Step Generator: generate the steps where **noise are insterted**\n",
    "* Noise Adder: Add noise in specific steps with specific $\\alpha$\n",
    "  * Theratically it is added step by step\n",
    "  * But We can compute the **cumulative product**\n",
    "* Noise Remover: remove input noise from noised figure with specific $\\alpha_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between DDIM and DDPM\n",
    "\n",
    "* Added nosie: the same: Gaussian noise step by step\n",
    "* The sequence of adding noise: dif \n",
    "  * DDPM: Each step + added from **no noise ($\\alpha=0$** to **very noisy $\\alpha = inf$**\n",
    "  * DDIM: sample step + added from **Noisy** to **no noise**\n",
    "* The inference: all from very **noisy** but\n",
    "  * DDPM: from steps **large to 0**, which means is **reverse** to input + Will add **random noise each time of inference**\n",
    "  * DDIM: **Deterministic**, also from steps **large to 0**, but sequence are the same with input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMNoiseSchduler(NoiseScheduler):\n",
    "    def __init__(self, beta_start, beta_end, timesteps):\n",
    "        self.beta = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_cum = torch.cumprod(self.alpha, dim=0)\n",
    "        self.timesteps = self.get_timesteps(timesteps)\n",
    "    def get_timesteps(self, steps):\n",
    "        return torch.arrange(steps)\n",
    "    def add_noise(self, x, noise, t):\n",
    "        pass \n",
    "        # same with template \n",
    "    def remove_noise(self, x, noise, t):\n",
    "        alpha = self.alpha_cum[t].view(-1,1,1,1)\n",
    "        beta = self.beta[t].view(-1,1,1,1)\n",
    "        \n",
    "        random_noise = torch.randn_like(x)\n",
    "        \n",
    "        return (x - torch.sqrt(1 - alpha) * noise) / torch.sqrt(alpha) + torch.sqrt(beta) * random_noise \n",
    "\n",
    "# * Training is the same\n",
    "# * Inference \n",
    "# TEST for t in reversed(range(timestpes)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMNoiseSchduler(NoiseScheduler):\n",
    "        def __init__(self, beta_start, beta_end, timesteps, sample_steps):\n",
    "        self.alpha = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alpha_cum = torch.cumprod(self.alpha, dim=0)\n",
    "        self.timesteps = self.get_timesteps(timesteps, sample_steps)\n",
    "        self.sample_steps = sample_steps\n",
    "    def get_timesteps(self, steps):\n",
    "        return torch.linspace(steps, 0, sample_steps)\n",
    "    def add_noise(self, x, noise, t):\n",
    "        pass \n",
    "        # same with template\n",
    "    def remove_noise(self, x, noise, t):\n",
    "        alpha = self.alpha_cum[t]\n",
    "        \n",
    "        return (x - torch.sqrt(1 - alpha) * noise) / torch.sqrt(alpha) \n",
    "    \n",
    "# * Training is the same\n",
    "# * Inference \n",
    "# TEST for t in range(timestpes):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write forward for generating a figure (Simulate pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will need\n",
    "\n",
    "* input text prompt \n",
    "* noise scheduler  \n",
    "* transformer\n",
    "* vae\n",
    "* image processor\n",
    "\n",
    "as input, then infer for **steps** to generate the figure\n",
    "\n",
    "The process is \n",
    "\n",
    "* get the inference steps based on scheduler and target steps\n",
    "* Get input noise(latent) compute by rand * sigma of noise scheduler\n",
    "* In timesteps loop\n",
    "  * get noised input\n",
    "  * pridict noise\n",
    "  * denoise by transformer\n",
    "* decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_images = []\n",
    "num_inference = 30\n",
    "guidance_scale = 7.5\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import retrieve_timesteps\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "timesteps, num_inference_steps = retrieve_timesteps(noise_scheduler, num_inference, accelerator.device)\n",
    "num_warmup_steps = max(len(timesteps) - num_inference_steps * noise_scheduler.order, 0)\n",
    "def prepare_latents(batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n",
    "    shape = (batch_size, num_channels_latents, int(height // vae.config.scaling_factor), int(width // vae.config.scaling_factor))\n",
    "    if isinstance(generator, list) and len(generator) != batch_size:\n",
    "        raise ValueError(\n",
    "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "        )\n",
    "    if latents is None:\n",
    "        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "    else:\n",
    "        latents = latents.to(accelerator.device)\n",
    "\n",
    "    # scale the initial noise by the standard deviation required by the scheduler\n",
    "    latents = latents * noise_scheduler.init_noise_sigma\n",
    "    return latents\n",
    "                \n",
    "                # latents = noisy_latents.to(torch.bfloat16)\n",
    "latent_channels = transformer.config.in_channels\n",
    "generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed is not None else None\n",
    "\n",
    "latents_inference = prepare_latents(\n",
    "    1,\n",
    "    latent_channels,\n",
    "    transformer.config.sample_size * vae.config.scaling_factor,\n",
    "    transformer.config.sample_size * vae.config.scaling_factor,\n",
    "    prompt_embeds.dtype,\n",
    "    device,\n",
    "    generator,\n",
    ")\n",
    "                    \n",
    "for i, t in enumerate(timesteps):\n",
    "    # latent_model_input = torch.cat([latents] * 2)\n",
    "    latent_model_input = noise_scheduler.scale_model_input(latents_inference, t)\n",
    "    current_timestep = t\n",
    "    if not torch.is_tensor(current_timestep):\n",
    "        is_mps = latent_model_input.device.type == \"mps\"\n",
    "        if isinstance(current_timestep, float):\n",
    "            dtype = torch.float32 if is_mps else torch.float64\n",
    "        else:\n",
    "            dtype = torch.int32 if is_mps else torch.int64\n",
    "        current_timestep = torch.tensor([current_timestep], dtype=dtype, device=latent_model_input.device)\n",
    "    elif len(current_timestep.shape) == 0:\n",
    "        current_timestep = current_timestep[None].to(latent_model_input.device)\n",
    "    current_timestep = current_timestep.expand(latent_model_input.shape[0])\n",
    "    # print(i)\n",
    "    # print(f\"latent {latent_model_input.dtype}; embeds {prompt_embeds_content.dtype}, time {current_timestep.dtype}\")\n",
    "    noise_pred = transformer(\n",
    "        latent_model_input,\n",
    "        encoder_hidden_states=prompt_embeds\n",
    "        encoder_attention_mask=prompt_attention_mask,\n",
    "        timestep=current_timestep,\n",
    "        added_cond_kwargs=added_cond_kwargs,\n",
    "        return_dict=False,\n",
    "    )[0] \n",
    "    # noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    # noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    if transformer.config.out_channels // 2 == latent_channels:\n",
    "        noise_pred = noise_pred.chunk(2, dim=1)[0]\n",
    "    else:\n",
    "        noise_pred = noise_pred\n",
    "\n",
    "    # compute previous image: x_t -> x_t-1\n",
    "    latents_inference = noise_scheduler.step(noise_pred, t, latents_inference, return_dict=False)[0]\n",
    "image = vae.decode(latents_inference.to(torch.bfloat16) / vae.config.scaling_factor, return_dict=False)[0]\n",
    "image_processor = VaeImageProcessor(vae_scale_factor=vae.config.scaling_factor)\n",
    "image = image_processor.postprocess(image, output_type=\"pil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parallel \n",
    "\n",
    "List how to implement data parallel by PyTorch. There are several ways, which are different in \n",
    "* Whether cross-device \n",
    "* Whether "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Data Parallel \n",
    "\n",
    "Distributed training, so need to use multiple **process** and use package related to **process communication**\n",
    "\n",
    "* Set Master process: define the main process to log, save checkpoints ... in `main (__name__)` function \n",
    "* multi processing launching: send the `running(selected by parameters) function(rank, ...)` to every process by `mp.spawn`, each process is numbered \n",
    "  * all process will run concurrently \n",
    "* Set up **inter-process connection** in the **running function**: `dist.init_process_group(\"gloo\"/\"nccl\", rank=rank, world_size=world_size)`\n",
    "* Set up **DDP** model\n",
    "  * the model structure\n",
    "  * The rank number it assigned `.to(rank)`\n",
    "* Move data to corresponding model `.to(rank)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parallel \n",
    "# * DP\n",
    "one_device_net = torch.nn.DataParallel(model, device_id=[0,1])\n",
    "# Following is the same\n",
    "\n",
    "# * DDP \n",
    "import dist \n",
    "import torch.multiprocessing as mp \n",
    "from torch.nn.parallel import  DistributedDataParallel as DDP \n",
    "\n",
    "def example(rank, world_size, model,loss, optimizer, input, label):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size = world_size)\n",
    "    model = model\n",
    "    ddp_model=DDP(model, device_id=[rank])\n",
    "    optimizer.zero_grad()\n",
    "    output = ddp_model(input.to(rank))\n",
    "    l = loss(output, label.to(rank))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "def main():\n",
    "    world_size = 4 \n",
    "    mp.spawn(example,\n",
    "             args=(world_size, args),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"1234\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually quantize tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_dequant_symmetric(x):\n",
    "    n_max = x.abs().max()\n",
    "    S = n_max / 127 \n",
    "    x_q = (x / scale).round().clamp(-127, 127)\n",
    "    x_dq = x_q * S \n",
    "    return x_q, x_dq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize with PyTorch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Tradition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
